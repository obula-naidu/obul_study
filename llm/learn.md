1Ô∏è‚É£ What is an LLM

A neural network trained to predict the next token based on previous tokens.
‚ÄúAn LLM is a neural network trained to predict the next token in a sequence; tokens are the atomic units of text the model processes.‚Äù

What an LLM is NOT

 It does not ‚Äúunderstand‚Äù like humans
 It does not store memory
 It does not know facts inherently
 It does not reason like logic engines

Input tokens
   ‚Üì
Neural network
   ‚Üì
Probability distribution of next token
   ‚Üì
Pick next token
   ‚Üì
Repeat


Tokens (not words)

Tokens are NOT words
Tokens are Pieces of text the model actually processes

One word ‚â† one token
One token ‚â† one word

Why tokens exist

LLMs:
Cannot process raw characters efficiently
Cannot process entire words reliably

So text is converted into tokens using a tokenizer.

Tokens control:
üîπ Cost (for cloud LLMs)
More tokens = more money
üîπ Context window
Models have a maximum number of tokens

Example:
Context window = 8,000 tokens

That includes:
System messages
User messages
Assistant replies

üîπ Speed
More tokens = slower response

Input tokens vs Output tokens

When you send:
Explain FastAPI in detail

You pay/use tokens for:
Input tokens (your prompt)
Output tokens (model‚Äôs answer)

Example breakdown

Prompt:
Explain FastAPI
‚âà 3 tokens
Response:
FastAPI is a modern, fast web framework...
‚âà 50 tokens

Total = ~53 tokens

Tokenization demo (conceptual)

Text:
ChatGPT is helpful

Becomes:
[1345, 2987, 203, 9876]

The model never sees words, only numbers.

Why LLMs feel ‚Äúsmart‚Äù

Because:
They‚Äôve seen trillions of token patterns
They predict the most likely continuation
They generate fluent language

But under the hood:
It‚Äôs next-token prediction, nothing more.

Stateless nature
LLMs do not store memory
Chat APIs do not keep history
Every request is independent

‚ÄúLLMs are stateless; conversational memory is simulated by resending context.‚Äù

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
2Ô∏è‚É£ Chat vs Generate 

Why chat ‚Äúfeels‚Äù stateful

Who stores memory

3Ô∏è‚É£ Message roles

system / user / assistant

Why system messages matter
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Student analogy

Parameters ‚Üí brain size (like llama3.2b here 3.2billion are parameters) 
        - ‚Äú3B parameters means the model has about 3 billion learned numerical weights that operate on tokenized inputs; they are not the tokens themselves.‚Äù
Training tokens ‚Üí books read
Context window ‚Üí short-term memory
Tokens ‚Üí words in a conversation

A student can:
Read millions of books
But still has one brain


Quantization is the process of reducing the precision of a model‚Äôs parameters to make LLMs smaller, faster, and cheaper to run.
Key points
LLMs have billions of parameters (numbers)
Normally stored as 32-bit floats (FP32) ‚Üí very large memory
Quantization stores them using fewer bits (16, 8, or 4)

Why it‚Äôs done

‚úÖ Reduces RAM / VRAM usage
‚úÖ Enables local inference (laptops, CPUs)
‚úÖ Faster loading and execution
‚ö†Ô∏è Slight quality loss (usually minor)

| Type | Bits per parameter | Approx memory |
| ---- | ------------------ | ------------- |
| FP32 | 32                 | ~12.8 GB      |
| FP16 | 16                 | ~6.4 GB       |
| INT8 | 8                  | ~3.2 GB       |
| Q4   | 4                  | ~1.6 GB       |
This is why Ollama models are often 2‚Äì4 GB, not 13 GB.

What quantization affects
‚úî Memory
‚úî Speed
‚úî Deployment feasibility

What it does not affect
‚ùå Context window
‚ùå Training data
‚ùå Model architecture

One-line takeaway (memorize this)
Quantization compresses model weights by reducing numerical precision, allowing large LLMs to run efficiently with minimal quality loss.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
4Ô∏è‚É£ Context window

Context window is the maximum number of tokens an LLM can see at one time in a single request.

Key points
*Measured in tokens, not words
*Includes:
    - System messages
    - User messages
    - Assistant replies
*Prompt + response together must fit inside it

What happens if it‚Äôs exceeded
*Oldest tokens are dropped (truncation), or
*Conversation is summarized, or
*Request fails (depends on system)

Why it exists
*LLMs use attention, which is computationally expensiv
*More tokens = more memory + slower inference

Important clarifications
‚ùå Tokens ‚â† context window
‚úî Context window = capacity limit
‚úî Tokens = content filling that limit

Why models ‚Äúforget‚Äù
*LLMs are stateless
*When tokens exceed the context window, earlier messages fall out
*System instructions can also be lost if too long

One-line takeaway (memorize this)
The context window is the maximum number of tokens an LLM can attend to in a single request; exceeding it causes loss of earlier context.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
5Ô∏è‚É£ Decoding parameters(temperature,top_p,max_tokens,stop)

Prompt
  ‚Üì
Tokenization
  ‚Üì
MODEL (3.2B learned weights)
  ‚Üì
Token probabilities
  ‚Üì
Decoding parameters (temperature, top_p, etc.)
  ‚Üì
Chosen token
  ‚Üì
Repeat


Decoding parameters are runtime controls that decide how an LLM selects the next token, without changing the model‚Äôs learned knowledge.

Model parameters = what the model knows
Decoding parameters = how it chooses words

What they are
*Applied after the model computes token probabilities
*Do not modify model weights
*Affect style, randomness, and length, not intelligence

| Parameter             | What it controls                       |
| --------------------- | -------------------------------------- |
| **temperature**       | Randomness of output                   |
| **top_p**             | Limits choices to most probable tokens |
| **max_tokens**        | Maximum tokens in response             |
| **stop**              | Tokens that end generation             |
| **presence_penalty**  | Encourages new topics                  |
| **frequency_penalty** | Reduces repetition                     |
| **seed**              | Makes output reproducible              |

1.Temperature ‚Äî Randomness of output

What it actually does
*Controls how sharp or flat the probability distribution is
*Applied before sampling

Intuition
*Low temperature ‚Üí model picks the most likely token
*High temperature ‚Üí model explores less likely tokens

| Temperature | Behavior               |
| ----------- | ---------------------- |
| 0.0‚Äì0.2     | Deterministic, factual |
| 0.3‚Äì0.7     | Balanced               |
| 0.8‚Äì1.2     | Creative               |
| >1.5        | Chaotic / nonsense     |

temperature = 0 ‚âà greedy decoding (no randomness)
Does not add new knowledge

2.top_p (nucleus sampling) ‚Äî Limits token choices

The nucleus is the smallest set of next-token candidates whose combined probability mass reaches top_p
At a given step, the model predicts probabilities for the next token only (not future tokens)
From that:
Tokens are sorted by probability
Added one by one
Stop when cumulative probability ‚â• top_p
That resulting set = nucleus

What it actually does
*Chooses the smallest set of tokens whose cumulative probability ‚â• top_p
*Sampling happens only inside this set

Intuition
*Prevents low-probability garbage tokens
*Keeps responses coherent

If probabilities are:
A: 0.50
B: 0.25
C: 0.15
D: 0.05
E: 0.05
-top_p = 0.9 ‚Üí {A, B, C}
-top_p = 0.6 ‚Üí {A, B}

| top_p | Behavior     |
| ----- | ------------ |
| 0.9   | Safe default |
| 0.7   | Conservative |
| 0.5   | Very strict  |

**top_k limits the model to choosing only the K most probable next tokens, regardless of their probabilities.(rarely used)
**Temperature reshapes probabilities; top_p selects from them ‚Äî so temperature must come first.

3.max_tokens ‚Äî Response length limit

What it actually does

*Hard upper bound on generated tokens
*Includes only the output, not prompt

Why it exists
*Prevents infinite generatio
*Controls cost & latency

Important clarifications
*If model finishes early ‚Üí stops naturally
*If limit is reached ‚Üí output is cut off

4.stop ‚Äî When generation must end

What it actually does
*Defines exact token sequences that force termination
*Checked after each token

Common use cases
*End at "User:"
*End at "###"

Stop before leaking system prompts
Example
"stop": ["\n\n", "User:"]

When model outputs any of these ‚Üí generation halts

Important
Stop tokens are not included in output
Multiple stop sequences allowed



Model computes probabilities
 ‚Üì
Temperature applied
 ‚Üì
top_p filtering
 ‚Üì
Token sampled
 ‚Üì
Check stop condition
 ‚Üì
Repeat until max_tokens or stop


Using high temperature + high top_p ‚Üí rambling
Very low max_tokens ‚Üí incomplete answers
Forgetting stop ‚Üí unwanted continuation


| Use case   | temperature | top_p | max_tokens |
| ---------- | ----------- | ----- | ---------- |
| Factual QA | 0.2         | 0.9   | 200        |
| Chatbot    | 0.6         | 0.9   | 300        |
| Creative   | 0.9         | 0.95  | 500        |
| Code       | 0.2         | 0.8   | 400        |

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
üëâ After Phase 1, nothing about LLM APIs will feel magical.

PHASE 2: Ollama (Local LLMs)

Goal: Comfort with local models

6Ô∏è‚É£ Ollama architecture

Ollama is a local HTTP server that loads LLM model files and exposes them via simple REST APIs.

Ollama = Local OpenAI server on your machine

*Instead of api.openai.com - You have localhost:11434
*Instead of cloud GPUs     - You use your CPU / GPU

What happens when you run this
ollama run llama3.2

Internally:
Ollama checks if model exists
If not:
    Downloads compressed model (zstd)
    Decompresses it
Loads model into memory
Starts inference loop
Accepts prompts

*Ollama ‚â† model
*Ollama = model runner + API server
*Model = .gguf file loaded by Ollama

Model files

Where Ollama stores models - Ollama stores models locally.
Typical locations: Linux: ~/.ollama/models
Inside, you‚Äôll see files like: llama3.2-3b.Q4_K_M.gguf

What a .gguf file contains

A .gguf file is not just weights.
It includes:
-Model architecture (layers, heads)
-Learned weights (quantized)
-Tokenizer & vocab
-Context window size
-Metadata

üìå Once loaded, Ollama does not need the internet.

zstd IS:

A compression format like zip
Used when downloading models - compress large model and decompress and store as gguf

Ollama downloads compressed models (zstd), stores them as quantized .gguf files, and loads them into memory for inference.

CPU vs GPU inference
Inference = Using already-trained weights to predict the next token

CPU inference (default)
How it works:

Uses highly optimized C++ (llama.cpp)
Uses:
    SIMD
    AVX / AVX2 / AVX512
Runs on normal CPU cores

GPU Inference
How it works:

Moves heavy matrix multiplications to GPU
Uses:
    CUDA (NVIDIA)
    Metal (Mac)
    ROCm (limited)

Ollama:
    Auto-detects GPU
    Automatically offloads layers
    Falls back to CPU if needed
You don‚Äôt manually choose CPU/GPU in most cases.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

7Ô∏è‚É£ Ollama APIs - refer to llm-basics.md

/api/chat - Multi-turn conversation,Role-based messages

/api/generate - Single prompt ‚Üí single response

/api/embeddings - text ‚Üí vectors

/api/tags - List installed models along with its metadata
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

8Ô∏è‚É£ Streaming responses - check in llm_basics.py 19-40

Token-by-token streaming
The model sends the response token by token (or chunk by chunk) instead of waiting for the full answer.
Without streaming:
    Tokens are generated
    Buffered
    Sent only after completion

With streaming:
    Tokens are sent as soon as they are generated

Why it matters for UX:
    Instant feedback
    Feels fast & alive
    Essential for chat appss
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

PHASE 3: OpenAI-style APIs (Cloud LLMs)

Goal: Switch providers without confusion
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
9Ô∏è‚É£ OpenAI ChatCompletion-style APIs

Same concepts, different URLs
only:
    URLs change
    Auth is added
    Limits exist
Ollama APIs and OpenAI APIs are conceptually the same.


Rate limits
Rate limits (new constraint)
Cloud models are shared.

Limits exist on:
    Requests per minute
    Tokens per minute
If exceeded ‚Üí errors (next task).

| Limit | Meaning             |
| ----- | ------------------- |
| RPM   | Requests per minute |
| TPM   | Tokens per minute   |


API keys
Why API keys exist
    Identify user
    Enforce billing
    Apply rate limits
How they are used
Authorization: Bearer sk-xxxx

Cloud LLM APIs do not change how LLMs work ‚Äî they only add authentication, billing, and limits.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
üîü Error handling & retries

Timeouts
A timeout means:
‚ÄúThe server didn‚Äôt respond within the time I‚Äôm willing to wait.‚Äù
| Type               | Where          |
| ------------------ | -------------- |
| Connection timeout | Network / DNS  |
| Read timeout       | Model is slow  |
| Client timeout     | Your SDK limit |


429, errors

Let‚Äôs say your limits are:
60 RPM
90,000 TPM

You send:
10 requests
Each = 10,000 tokens (input + output)
‚ùå 100,000 tokens ‚Üí 429 error
Even though RPM is OK.

What triggers 429 (Rate Limit)?
Common causes
    Too many parallel requests
    Large prompts
    Streaming many tokens
    Agent loops

{
  "error": {
    "code": "rate_limit_exceeded",
    "message": "Too many requests"
  }
}

Bad code (naive)

for query in queries:
    call_llm(query)

Exponential backoff
Good code (with retry + backoff)
import time
import random

def call_with_retry(fn, retries=5):
    for i in range(retries):
        try:
            return fn()
        except RateLimitError:
            sleep = (2 ** i) + random.random()
            time.sleep(sleep)
    raise Exception("Max retries exceeded")

Why exponential backoff works

If everyone retries immediately ‚Üí thundering herd
1s ‚Üí 2s ‚Üí 4s ‚Üí 8s ‚Üí success
Allows:

Token bucket to refill
Queue to clear


500-level errors = server-side failures
| Code | Meaning             |
| ---- | ------------------- |
| 500  | Internal error      |
| 502  | Bad gateway         |
| 503  | Service unavailable |
| 504  | Gateway timeout     |

import time
import random

def call_llm_with_retry(fn, retries=5):
    for i in range(retries):
        try:
            return fn()
        except (TimeoutError, ServerError) as e:
            sleep = min(60, (2 ** i) + random.random())
            time.sleep(sleep)
    raise Exception("LLM unavailable")

| Error   | Retry? | Delay              |
| ------- | ------ | ------------------ |
| Timeout | ‚úÖ      | Exponential        |
| 500     | ‚úÖ      | Exponential        |
| 502/503 | ‚úÖ      | Longer             |
| 429     | ‚úÖ      | Provider-specified |
| 400     | ‚ùå      | Fix request        |
| 401     | ‚ùå      | Fix auth           |



--------------------------------------------------------------------------------------------------------------------------------------------------------------------
PART A ‚Äî Embeddings (Foundation)

11.1 What embeddings are and token vs embeddings
An embedding is a numerical vector that represents meaning.

Not text.
Not tokens.
Meaning.

Why embeddings exist

LLMs:

Generate text
Are bad at searching large knowledge bases

Embeddings:
    Convert text ‚Üí numbers
    Enable similarity search

Example (conceptual)
"FastAPI is a Python framework"
‚Üí [0.13, -0.44, 0.82, ..., 0.09]  (1536 dimensions)
These vectors are close in space.

What vectors actually encode

They encode:
    Topic
    Intent
    Semantics
    Relationships

They do ‚ùå NOT encode:
    Grammar
    Exact wording
    Order (mostly)

| Concept    | Purpose            |
| ---------- | ------------------ |
| Tokens     | Generation         |
| Embeddings | Search & retrieval |

Embeddings turn meaning into geometry.

‚ùå Embeddings are not model parameters
‚ùå Embeddings are not learned per query
‚ùå Embeddings are not context windows
They are outputs of a trained embedding model.


Where embeddings live in RAG
Docs ‚Üí embeddings ‚Üí vector DB
Query ‚Üí embedding ‚Üí similarity search
Top chunks ‚Üí prompt ‚Üí LLM

Where embeddings live in RAG
Docs ‚Üí embeddings ‚Üí vector DB
Query ‚Üí embedding ‚Üí similarity search
Top chunks ‚Üí prompt ‚Üí LLM

Why embeddings make LLMs ‚Äúknow things‚Äù
LLMs:
    Don‚Äôt store your documents
RAG:
    Retrieves relevant docs at runtime
    Injects them into prompt

Knowledge is externalized

11.2 Embedding models and Dimensionality

What is an embedding model?
An embedding model is a neural network trained to map text ‚Üí vectors such that semantic similarity = geometric closeness.
üìå Different from a chat/generation model.

How embedding models are trained (conceptual)

They are trained on:
    Sentence pairs
    Question‚Äìanswer pairs
    Paraphrases
    Contrastive learning

Training goal:
    similar meaning ‚Üí vectors close
    different meaning ‚Üí vectors far

Types of embedding models
üîπ Proprietary (Cloud)
    OpenAI text-embedding-3-large
    Cohere
    Google

üîπ Open-source
    SentenceTransformers
    BGE (BAAI)
    E5
    GTE
    Instructor models

üîπ Local (Ollama)
    nomic-embed-text
    mxbai-embed-large
    bge-base

Dimensionality
| Model        | Dimensions |
| ------------ | ---------- |
| OpenAI small | 768        |
| OpenAI large | 3072       |
| BGE-base     | 768        |
| BGE-large    | 1024       |
| nomic        | 768        |

| Lower dim   | Higher dim    |
| ----------- | ------------- |
| Faster      | More accurate |
| Less memory | Better nuance |
| Cheaper     | Slower        |
768‚Äì1024 is the industry sweet spot.

One crucial rule (people mess this up)
Query and documents MUST use the same embedding model.
Mixing models = broken similarity search.

Embedding normalization

Most models output vectors that are:
    Already normalized OR
    Should be normalized
Why?
    Makes cosine similarity stable
    Improves indexing
Many vector DBs auto-normalize.

When embeddings FAIL
    Very short queries (‚Äúyes‚Äù, ‚Äúok‚Äù)
    Exact keyword search
    Numbers / IDs
    Highly structured data

Use hybrid search (later topic).

Mental model
    Embedding model = semantic encoder
    Vector DB = memory
    LLM = reasoning engine

11.4 Similarity metrics (cosine, dot, L2)
Why similarity metrics exist
Once you have embeddings (vectors), you need to answer:
    ‚ÄúHow close are these two meanings?‚Äù
    That‚Äôs what similarity metrics do.

1Ô∏è‚É£ The three main similarity metrics
| Metric                  | Used for       |
| ----------------------- | -------------- |
| Cosine similarity       | Most common    |
| Dot product             | Fast, ranking  |
| L2 (Euclidean) distance | Geometry-based |

Cosine similarity (most important)
Measures the angle between two vectors, not their length.

Why cosine is king üëë
Ignores magnitude
Focuses on direction (meaning)
Stable across embedding models
Works well with normalized vectors

üìå Most embedding models are trained expecting cosine similarity.

| Cosine value | Meaning           |
| ------------ | ----------------- |
| 1.0          | Identical meaning |
| 0.8          | Very similar      |
| 0.5          | Somewhat related  |
| 0.0          | Unrelated         |
| -1.0         | Opposite meaning  |

Dot product
Measures both direction and magnitude

When dot product is used
    Vectors are normalized
    Speed is critical
    Ranking is more important than exact similarity

üìå If vectors are normalized:
dot product ‚âà cosine similarity
That‚Äôs why some DBs use dot product internally.

2 (Euclidean) distance
Straight-line distance between vectors

Why L2 is less popular
    Sensitive to magnitude
    Worse semantic behavior
    Less aligned with training objectives

Used mostly in:
    Vision models
    Older embedding systems

Why cosine works best for text

Text meaning is:
    Directional
    Relative
    Scale-independent

Cosine captures exactly that.

Vector DB perspective

Most vector DBs support all metrics, but:
| DB       | Default       |
| -------- | ------------- |
| FAISS    | Inner product |
| Chroma   | Cosine        |
| Pinecone | Cosine        |
| Weaviate | Cosine        |

Common mistake (critical)

‚ùå Using cosine on non-normalized vectors
‚ùå Mixing similarity metrics between indexing & querying

üìå Index metric == query metric (must match).

Meaning = direction
Similarity = angle
Cosine = angle comparison

What ‚Äúdimension‚Äù really means
    One dimension = one learned semantic feature
An embedding of size 768 means:
    768 independent semantic signals
Not human-interpretable, but statistically meaningful.

Why embeddings have FIXED size
Neural networks require:
    Fixed input size
    Fixed output size
So:
    Any text length ‚Üí same-size vector

That‚Äôs why:
1 sentence
1 paragraph
1 page
All become 768 numbers (for that model).

Why not 10 dimensions? Why not 1 million?
Too few dimensions
    Can‚Äôt represent nuance
    Many meanings collapse together
    Poor retrieval quality

Too many dimensions
    Slow search
    High memory
    Harder indexing
    Diminishing returns

Why 768 / 1024 became standard
These numbers come from:
    Transformer hidden sizes
    Powers of 2
    Hardware efficiency

Example:

| Model family | Hidden size |
| ------------ | ----------- |
| BERT-base    | 768         |
| RoBERTa      | 768         |
| BGE-base     | 768         |
| BGE-large    | 1024        |


üìå Embedding head often mirrors hidden size.

Curse of dimensionality (important)

As dimensions increase:
    Distance between points becomes less meaningful
    Everything starts to look ‚Äúfar‚Äù
    Indexing gets harder

Vector DBs combat this with:
    Approximate nearest neighbor (ANN)
    Quantization
    Clustering

Practical tradeoffs
| Use case              | Recommended dims |
| --------------------- | ---------------- |
| Small app             | 384‚Äì768          |
| RAG systems           | 768‚Äì1024         |
| High-precision search | 1024‚Äì1536        |
| Edge / mobile         | ‚â§384             |

Can you reduce dimensions?
Yes:
    PCA
    Autoencoders
    Quantization

But:
‚ùå usually hurts retrieval
‚úîÔ∏è useful for memory-constrained systems

Mental model

Dimensions = semantic resolution
More dims = sharper meaning
Fewer dims = blurrier meaning

When Embeddings FAIL
This task explains why RAG systems sometimes give bad answers even with embeddings.

Embeddings are semantic, not factual.
They capture meaning similarity, not truth or exactness.

Major failure cases
1. Very short queries
Examples:
‚Äúyes‚Äù
‚Äúok‚Äù
‚Äúwhy?‚Äù
‚û°Ô∏è Too little semantic signal
‚û°Ô∏è Vectors are noisy
Fix: Expand query or use conversation context.

2. Keyword-heavy queries
Examples:
    Error codes (ERR_CONN_RESET)
    IDs (order_839201)
    File names
Embeddings blur exact tokens.
Fix: Keyword search or hybrid search.

3. Numerical & tabular data
Examples:
    Prices
    Dates
    Metrics
Embeddings don‚Äôt preserve numeric precision.
Fix: Structured DB + RAG.

4. Domain mismatch
Embedding model not trained on:
    Telecom logs
    Kernel traces
    Medical codes
‚û°Ô∏è Similarity becomes meaningless.
Fix: Domain-specific embeddings.

5. Long documents, bad chunking
    Important info split across chunks
    Context lost
Fix: Smarter chunking (next section).

6. False positives (semantic drift)
Query:
‚ÄúHow to reset router?‚Äù
Retrieved:
‚ÄúHow to restart application‚Äù

Semantically similar but wrong.

Why embeddings don‚Äôt ‚Äúunderstand‚Äù
They optimize:
similar meaning ‚Üí close vectors

They do NOT optimize:
    Logical correctness
    Temporal truth
    Causality
That‚Äôs LLM‚Äôs job.

4Ô∏è‚É£ Warning sign in RAG

If your RAG answers:
    Confidently wrong
    With irrelevant citations
‚û°Ô∏è Retrieval is broken, not generation.

Embeddings vs search engines
| Feature     | Embeddings | Keyword search |
| ----------- | ---------- | -------------- |
| Semantic    | ‚úÖ          | ‚ùå              |
| Exact match | ‚ùå          | ‚úÖ              |
| Numbers     | ‚ùå          | ‚úÖ              |
| Speed       | Medium     | Fast           |

‚û°Ô∏è That‚Äôs why hybrid search exists.

 Mental model
Embeddings = fuzzy semantic lens
Keyword search = sharp literal lens

Best systems use both.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

PART B ‚Äî Vector Databases
------------------------

Why do we even need Vector Databases?

LLMs don‚Äôt ‚Äúsearch text‚Äù ‚Äî they compare meanings.
When you embed text, you convert it into a vector

Now imagine:
1 question embedding
1 million document embeddings

To answer:
üëâ ‚ÄúWhich documents are semantically closest?‚Äù

That is vector similarity search, not keyword search.

What happens without a vector DB?
for each vector in database:
    compute similarity
O(N √ó D)
N = number of vectors
D = embedding dimensions
With:
N = 1,000,000
D = 768
‚ùå Too slow
‚ùå Too expensive
‚ùå Not scalable
What a Vector DB gives you
A vector database:
    Stores embeddings efficiently
    Builds ANN indexes (Approximate Nearest Neighbor)
    Finds top-K closest vectors in milliseconds
    Handles metadata filtering
    Scales to millions/billions of vectors

Key idea
We trade perfect accuracy for massive speed gains

Why SQL / Traditional DBs aren‚Äôt enough
SQL is great at:
| Task                   | Works? |
| ---------------------- | ------ |
| `WHERE age > 30`       | ‚úÖ      |
| `JOIN users ON orders` | ‚úÖ      |
| Exact match            | ‚úÖ      |
| Range queries          | ‚úÖ      |

SQL is terrible at:
| Task                      | Why                |
| ------------------------- | ------------------ |
| Semantic similarity       | No vector math     |
| High-dim cosine search    | No native ANN      |
| Scaling similarity search | Full table scans   |
| Top-K nearest neighbors   | Brutal performance |
Even if you store vectors as arrays:
SELECT * FROM docs
ORDER BY cosine_similarity(vec, query_vec)
LIMIT 5;
This forces full scan every time.

| Traditional DB  | Vector DB            |
| --------------- | -------------------- |
| Structured data | Unstructured meaning |
| Exact logic     | Fuzzy similarity     |
| Rows & columns  | Points in space      |
| Deterministic   | Probabilistic        |


What is ANN (Approximate Nearest Neighbor)?
Exact Nearest Neighbor (ENN)
    Computes distance to every vector
    Accurate
    Unusable at scale

ANN (what everyone uses)
    Uses clever indexing tricks
    Narrows search space
    Finds very close neighbors (not mathematically perfect)
    100√ó‚Äì1000√ó faster

Important truth
In RAG, ‚Äúalmost correct‚Äù retrieval is MORE than enough
LLMs are robust to slight noise.

ANN Indexing ‚Äî Core intuition (NO math yet)
Imagine vectors as points in space.
ANN tries to answer:
‚ÄúWhich small region of space should I search instead of everything?‚Äù

Common ANN ideas (high-level)
1Ô∏è‚É£ Space partitioning
    Divide vector space into regions
    Only search nearby regions

2Ô∏è‚É£ Graph-based navigation
    Each vector links to neighbors
    Traverse graph from entry point

3Ô∏è‚É£ Clustering
    Group similar vectors
    Search only top clusters

Why ANN indexing is a separate step
Text ‚Üí Embedding ‚Üí Normalize ‚Üí Store ‚Üí Index ‚Üí Search

Key rules:
    Indexing is built after vectors are stored
    Index depends on:
        Distance metric (cosine / L2 / dot)
        Vector dimension
        Dataset size

If you:
    Change embedding model ‚ùå
    Change normalization ‚ùå
    Change distance metric ‚ùå
‚û°Ô∏è Rebuild 

Types of Vector Databases (preview)

FAISS ‚Üí fastest, in-memory
Chroma ‚Üí dev-friendly, metadata
Pinecone ‚Üí managed, scalable
Weaviate ‚Üí schema + hybrid search
Milvus ‚Üí massive scale
PostgreSQL + pgvector ‚Üí SQL + vectors

Indexing Methods (ANN Deep Dive)
You have:
    N vectors (documents)
    1 query vector
    A distance metric (cosine / L2 / dot)
Goal:
    Find top-K closest vectors without scanning all N
Indexing answers:
    ‚ÄúWhich small subset of vectors should I even look at?‚Äù

1Ô∏è‚É£ Flat Index (Baseline ‚Äî No ANN)
What it is
    Store vectors as-is
    On search ‚Üí compare against every vector
    Complexity:O(N √ó D)

Pros
‚úÖ Exact results
‚úÖ Simple
‚úÖ No preprocessing
Cons
‚ùå Extremely slow at scale
‚ùå No pruning

When it‚Äôs used
    N < 10k
    Evaluation / testing
    Gold-standard accuracy checks

üìå Important:
Every ANN index is compared against Flat for accuracy

2Ô∏è‚É£ IVF ‚Äî Inverted File Index (Clustering-based)
Core idea
‚ÄúDon‚Äôt search everywhere ‚Äî search only relevant clusters‚Äù
Step 1: Train centroids
    Run k-means on vectors
    Produce nlist centroids
Step 2: Assign vectors
    Each vector goes to nearest centroid.
        Centroid A ‚Üí [v1, v7, v103]
        Centroid B ‚Üí [v2, v9, v55]
Step 3: Query time
    Embed query
    Find nearest nprobe centroids
    Search only vectors inside those centroids

| Parameter | Meaning                     |
| --------- | --------------------------- |
| `nlist`   | Number of clusters          |
| `nprobe`  | How many clusters to search |

Pros
‚úÖ Massive speedup
‚úÖ Good for millions of vectors
‚úÖ Tunable accuracy
Cons
‚ùå Needs training
‚ùå Bad if clusters are poor
‚ùå Recall depends on nprobe

Where IVF shines
    Large static datasets
    Embeddings don‚Äôt change often
    Disk-backed indexes

3Ô∏è‚É£ HNSW ‚Äî Hierarchical Navigable Small World Graph
Core idea
‚ÄúVectors form a graph; similar vectors are neighbors‚Äù

Instead of clustering, HNSW:
    Builds a multi-layer graph
    Higher layers = fewer nodes
    Lower layers = dense connections

Layer 3 (sparse)
   ‚Üì
Layer 2
   ‚Üì
Layer 1
   ‚Üì
Layer 0 (dense, full graph)

Query process
    Start at top layer
    Greedily move to closer neighbors
    Drop down layers
    Final fine search at bottom
üöÄ No clustering. No scanning. Just graph traversal.

| Parameter        | Meaning                      |
| ---------------- | ---------------------------- |
| `M`              | Number of neighbors per node |
| `efConstruction` | Index build quality          |
| `efSearch`       | Search accuracy vs speed     |

Bigger efSearch = better recall
Pros
‚úÖ Extremely fast
‚úÖ High recall
‚úÖ No training step
‚úÖ Dynamic inserts supported
Cons
‚ùå High memory usage
‚ùå Complex internals

HNSW is the default choice unless you have a strong reason not to
Most modern vector DBs use HNSW internally.

4Ô∏è‚É£ PQ ‚Äî Product Quantization (Compression)
Core idea
‚ÄúStore approximate vectors using fewer bytes‚Äù

How PQ works
    Split vector into sub-vectors
    Quantize each part separately
    Store codes instead of floats
Example:
    768-d float vector ‚Üí ~3KB
    PQ compressed ‚Üí ~64‚Äì128 bytes

Why PQ exists
    Memory is expensive
    Disk I/O is slow
    PQ allows billions of vectors

| Aspect   | Result          |
| -------- | --------------- |
| Memory   | üî• Huge win     |
| Speed    | üî• Faster cache |
| Accuracy | ‚ùå Some loss     |

PQ is rarely used alone
Usually combined with IVF: IVF+PQ

Hybrid indexes
| Combo          | Why                   |
| -------------- | --------------------- |
| Flat           | Ground truth          |
| IVF            | Large datasets        |
| HNSW           | High recall + speed   |
| IVF + PQ       | Massive scale         |
| HNSW + filters | Metadata-aware search |

Distance metrics & indexing compatibility
This is critical.
| Metric      | Notes                         |
| ----------- | ----------------------------- |
| Cosine      | Requires normalization        |
| Dot product | Often with normalized vectors |
| L2          | Raw vectors                   |

FAISS is NOT just a vector DB.
It is a library of ANN indexes.
It provides:
    Flat
    IVF
    PQ
    IVFPQ
    HNSW
Most vector databases internally use FAISS or FAISS-like algorithms.

Accuracy vs speed vs memory triangle
You can only optimize two:
| Optimize          | Sacrifice |
| ----------------- | --------- |
| Speed + Accuracy  | Memory    |
| Speed + Memory    | Accuracy  |
| Accuracy + Memory | Speed     |
ANN is engineering tradeoffs, not magic.

FAISS
(In-memory ANN engine, performance king)
What exactly is FAISS?

FAISS = Facebook AI Similarity Search
Important correction to lock in:
    FAISS is NOT a database
    FAISS is an ANN indexing + search library

It does:
    Vector storage (in RAM / mmap)
    ANN indexing
    Ultra-fast similarity search

It does NOT:
    Handle metadata well
    Do filtering
    Handle persistence like a DB
    Provide auth / scaling / replication
FAISS = engine, not platform.

Why FAISS exists (the real reason)
Before FAISS:
    Academic ANN code
    Inconsistent performance
    No GPU support
    Hard to scale beyond millions

FAISS solved:
    High-dimensional similarity search
    CPU + GPU acceleration
    Pluggable index types
    Production-grade speed
Today: Almost every vector DB is either built on FAISS or re-implements its ideas

FAISS architecture
Embeddings (float vectors)
        ‚Üì
FAISS Index
   ‚îú‚îÄ Flat
   ‚îú‚îÄ IVF
   ‚îú‚îÄ HNSW
   ‚îú‚îÄ PQ / IVFPQ
        ‚Üì
Top-K IDs + distances

FAISS does only one thing:
Given a query vector ‚Üí return nearest vector IDs

Core FAISS index types (what actually matters)
1Ô∏è‚É£ IndexFlat (Exact)
        IndexFlatL2
        IndexFlatIP
    No ANN
    Exact search
    Baseline
Use when:
    Small dataset
    Measuring recall

2Ô∏è‚É£ IndexIVFFlat
        IndexIVFFlat(quantizer, d, nlist)
    IVF clustering
    Flat vectors inside clusters
Key params:
    nlist ‚Üí number of clusters
    nprobe ‚Üí clusters searched at query

Good for:
    Millions of vectors
    Disk-backed indexes

3Ô∏è‚É£ IndexHNSWFlat
        IndexHNSWFlat(d, M)
    Graph-based ANN
    Very fast
    High recall
Use when:
    Low latency matters
    RAM is available
    Dynamic inserts needed

4Ô∏è‚É£ IndexIVFPQ (Scale monster)
        IndexIVFPQ(d, nlist, m, nbits)
    IVF + Product Quantization
    Massive compression
Use when:
    Tens / hundreds of millions
    Memory constrained

FAISS + Distance metrics (CRITICAL)
FAISS supports:
    L2
    Inner Product (dot)
üìå Cosine similarity is NOT native
Cosine trick:
    Normalize vectors ‚Üí use dot product

If you forget normalization:
‚ùå Garbage results
‚ùå Silent failure (no error)

GPU FAISS (why it‚Äôs special)
FAISS GPU:
    Runs ANN search on GPU
    Blazing fast for large batches
    Used by Meta, Google-scale systems

Tradeoff:
    GPU memory limits
    Transfer overhead
    Harder to deploy

Used when:
    High QPS
    Batch queries
    Embedding pipelines on GPU

FAISS lifecycle in real RAG systems
    Typical flow
    1. Embed documents
    2. Normalize embeddings
    3. Add to FAISS index
    4. Build index (train if needed)
    5. Save index to disk
At query time:
    Query ‚Üí embed ‚Üí normalize ‚Üí FAISS.search ‚Üí IDs

Why FAISS is NOT enough alone
| Feature            | FAISS     |
| ------------------ | --------- |
| Metadata filtering | ‚ùå         |
| Persistence        | ‚ö†Ô∏è manual |
| Distributed search | ‚ùå         |
| REST API           | ‚ùå         |
| Multi-tenant       | ‚ùå         |


That‚Äôs why people wrap FAISS inside:
Chroma
Weaviate
Milvus
Custom services

When should YOU use FAISS directly?
Use FAISS if:

‚úÖ You want maximum performance
‚úÖ You control the infrastructure
‚úÖ You‚Äôre okay writing glue code
‚úÖ You don‚Äôt need complex filters

Avoid FAISS if:

‚ùå You want fast prototyping
‚ùå You need metadata filters
‚ùå You want managed scaling

FAISS vs Vector DBs (truth table)
| Feature        | FAISS | Vector DB |
| -------------- | ----- | --------- |
| ANN algorithms | ‚úÖ     | ‚úÖ         |
| Metadata       | ‚ùå     | ‚úÖ         |
| Persistence    | ‚ö†Ô∏è    | ‚úÖ         |
| Scaling        | ‚ùå     | ‚úÖ         |
| Ease of use    | ‚ùå     | ‚úÖ         |


FAISS is the engine
Vector DBs are the cars built on it

üîë Key takeaways
FAISS = gold standard ANN library
In-memory, ultra-fast
Requires discipline (normalization, index rebuilds)
Forms the foundation of many vector DBs
Best for performance-critical systems

Chroma DB
(Developer-first vector database)

1Ô∏è‚É£ What is Chroma?
Chroma is an open-source vector database designed for:
    Local development
    Prototyping RAG systems
    Metadata-heavy workflows
    Tight integration with LLM frameworks
Think of Chroma as:
    FAISS + persistence + metadata + DX

2Ô∏è‚É£ Why Chroma exists
FAISS problems:
    No metadata filtering
    No persistence by default
    No simple API
    Easy to misuse
Chroma solves:
    Simple Python API
    Automatic persistence
    Metadata storage
    Filters (where, where_document)
    Seamless LangChain / LlamaIndex usage

3Ô∏è‚É£ Chroma architecture (conceptual)
Documents + Metadata
        ‚Üì
Embedding Function
        ‚Üì
Chroma Collection
   ‚îú‚îÄ Vectors
   ‚îú‚îÄ Metadata index
   ‚îú‚îÄ ANN index (HNSW)
        ‚Üì
Similarity Search + Filters

Internally:
    Uses HNSW
    Uses SQLite / DuckDB for metadata
    Stores vectors on disk

4Ô∏è‚É£ Core concepts in Chroma (very important)
    1Ô∏è‚É£ Collection
    A collection = logical namespace
    Example:
    collection = chroma_client.create_collection("docs")

    Rule:
    One embedding model per collection
    Mixing embeddings = broken similarity.

    2Ô∏è‚É£ Documents
    Raw text chunks
    documents = ["Reset the router...", "Check power cable..."]

    3Ô∏è‚É£ Metadata
    Structured filters
    metadata = [
    {"source": "manual", "page": 4},
    {"source": "faq", "page": 1}
    ]

    4Ô∏è‚É£ IDs
    User-defined or auto-generated
    IDs must be:
        Unique
        Stable (important for updates)

5Ô∏è‚É£ How Chroma search works
Query pipeline
    Query text
    ‚Üì
    Embedding
    ‚Üì
    HNSW ANN search
    ‚Üì
    Metadata filter
    ‚Üì
    Top-K documents


Filtering happens after ANN narrowing, not before.

6Ô∏è‚É£ Why metadata filtering matters (RAG reality)
Real RAG questions:
    ‚ÄúOnly search logs from last week‚Äù
    ‚ÄúOnly config files‚Äù
    ‚ÄúOnly RU alarms‚Äù

Without metadata:
    ‚ùå Irrelevant chunks pollute context
    ‚ùå LLM hallucinations increase

Chroma makes metadata first-class, not an afterthought.

7Ô∏è‚É£ Persistence model (what actually happens)

Chroma: 
    Writes vectors + metadata to disk
    Reloads on restart
    No manual save/load needed
Tradeoff:
    Slower than raw FAISS
    But far safer

8Ô∏è‚É£ Strengths of Chroma

‚úÖ Very easy to use
‚úÖ Great for RAG experiments
‚úÖ Metadata filtering built-in
‚úÖ Open source
‚úÖ Plays well with LangChain

9Ô∏è‚É£ Limitations (important to know)

‚ùå Not designed for massive scale
‚ùå Single-node focus
‚ùå Limited index customization
‚ùå Not ideal for high-QPS production

Chroma is:
Dev & prototype DB ‚Äî not infra-grade

üîü Chroma vs FAISS (practical view)
Aspect	FAISS	Chroma
Speed	üî•üî•üî•	üî•üî•
Metadata	‚ùå	‚úÖ
Persistence	‚ùå	‚úÖ
Ease of use	‚ùå	‚úÖ
Scale	Huge (manual)	Small‚ÄìMedium

1Ô∏è‚É£1Ô∏è‚É£ When should YOU use Chroma?
Use Chroma if:
    You‚Äôre learning RAG
    You want quick iteration
    Dataset < few million chunks
    Metadata matters
Avoid Chroma if:
    You need distributed search
    You need strict latency SLOs
    You expect heavy concurrency

üîë Key takeaways

Chroma = developer-first vector DB
Built on ANN (HNSW)
Strong metadata support
Perfect for learning & prototyping
Not a FAISS replacement ‚Äî a wrapper


Vector DB Alternatives
(Why so many exist & what problem each solves)
All vector DBs solve similarity search, but they optimize different constraints:
    Scale
    Cost
    Dev experience
    Filtering
    Cloud vs self-hosted

1Ô∏è‚É£ Pinecone ‚Äî Managed & production-ready
Pinecone
What Pinecone is
    Fully managed vector DB (SaaS)
    No infra to manage
    Built for production RAG
Why Pinecone exists
Problem:
    FAISS & open-source DBs need ops
    Scaling ANN is hard
    High availability is painful
Pinecone solves:
    Auto-scaling
    Replication
    Backups
    High QPS(queries per second)
    Global availability

Strengths
    ‚úÖ Zero infra
    ‚úÖ High reliability
    ‚úÖ Metadata filtering
    ‚úÖ Fast ANN
    ‚úÖ Good for startups
Weaknesses
    ‚ùå Cost
    ‚ùå Black-box internals
    ‚ùå Vendor lock-in

When to use Pinecone
    Production RAG
    Customer-facing apps
    Teams without infra expertise

2Ô∏è‚É£ Weaviate ‚Äî Schema + Hybrid Search
Weaviate
What Weaviate is
    Open-source vector DB
    Strong schema support
    Built-in hybrid search

Key differentiator: Hybrid search
    Keyword score + Vector score

This helps when:
    Keywords matter (IDs, error codes)
    Semantic search alone fails

Strengths
    ‚úÖ Hybrid search
    ‚úÖ Rich metadata filtering
    ‚úÖ Graph-like schema
    ‚úÖ Self-host or cloud

Weaknesses
    ‚ùå More complex
    ‚ùå Higher learning curve
    ‚ùå Slower than pure ANN engines

When to use Weaviate
    Mixed keyword + semantic search
    Knowledge graphs
    Enterprise schemas

3Ô∏è‚É£ Milvus ‚Äî Massive scale (infra-grade)
Milvus
What Milvus is
    Distributed vector DB
    Designed for billions of vectors
    CNCF-backed
Why Milvus exists
Single-node DBs fail when:
    Dataset grows huge
    RAM is limited
    QPS explodes
Milvus solves:
    Sharding
    Replication
    Disk-based ANN
    Cloud-native deployments
Strengths
    ‚úÖ Extreme scale
    ‚úÖ Multiple ANN algorithms
    ‚úÖ Cloud-native
    ‚úÖ Used by large enterprises
Weaknesses
    ‚ùå Heavy infra
    ‚ùå Complex ops
    ‚ùå Overkill for most RAGs
When to use Milvus
    Very large corpora
    Enterprise / telecom scale
    Strict SLA systems

4Ô∏è‚É£ pgvector ‚Äî SQL + vectors
PostgreSQL + pgvector extension
What pgvector is
    Vector type inside PostgreSQL
    ANN indexes (HNSW, IVF)
    SQL-first approach
Why pgvector exists
Problem:
    Teams already use PostgreSQL
    Don‚Äôt want new DBs
pgvector gives:
    Vectors in SQL
    Metadata joins
    Transactions
Strengths
    ‚úÖ Familiar SQL
    ‚úÖ ACID guarantees
    ‚úÖ Easy integration
    ‚úÖ Good for small‚Äìmedium scale
Weaknesses
    ‚ùå Slower than dedicated engines
    ‚ùå Limited ANN tuning
    ‚ùå Not ideal for massive scale

When to use pgvector
    Existing PostgreSQL infra
    Moderate vector counts
    Heavy relational metadata

5Ô∏è‚É£ Comparison table (truth, not marketing)
| DB       | Best at         | Scale     | Ops    |
| -------- | --------------- | --------- | ------ |
| FAISS    | Raw speed       | Huge      | Manual |
| Chroma   | Dev & RAG       | Small‚ÄìMed | Easy   |
| Pinecone | Prod SaaS       | Large     | Zero   |
| Weaviate | Hybrid search   | Large     | Medium |
| Milvus   | Massive scale   | Huge      | Heavy  |
| pgvector | SQL integration | Small‚ÄìMed | Easy   |

6Ô∏è‚É£ Choosing wrong = bad RAG
Common mistakes:
    Pinecone for tiny experiments ‚Üí üí∏
    Chroma for high-QPS prod ‚Üí üî•
    Milvus for prototypes ‚Üí üòµ
    FAISS without metadata ‚Üí ü§Ø

üîë Key takeaways
No ‚Äúbest‚Äù vector DB
Choose based on:
    Scale
    Metadata needs
    Infra maturity
    Budget
ANN algorithm matters more than branding



When to Use What
(Practical decision framework for Vector DBs)
This section connects:
    Scale
    Latency
    Cost
    Metadata
    Team maturity
No theory. Only real-world choices.

1Ô∏è‚É£ First question: How big is your data?
A) Small (‚â§ 100k vectors)
    Experiments
    Learning RAG
    Local tools
    ‚úÖ Best choices:
        Chroma
        PostgreSQL + pgvector
        FAISS Flat
    ‚ùå Avoid:
        Milvus
        Pinecone

B) Medium (100k ‚Äì 10M vectors)
    Internal tools
    Knowledge bases
    AI assistants
    ‚úÖ Best choices:
        FAISS (HNSW / IVF)
        Chroma (upper bound)
        pgvector (carefully)
        Weaviate

C) Large (10M ‚Äì 1B+ vectors)
    Search platforms
    Customer-facing RAG
    Enterprise AI
    ‚úÖ Best choices:
        Pinecone
        Milvus
        FAISS + custom infra

2Ô∏è‚É£ Second question: Do you need metadata filtering?
‚ùå Minimal metadata
Just similarity search
Use:
    FAISS
    Pinecone (basic filters)
‚úÖ Heavy metadata filters
Examples:
    source type
    time range
    severity
    component ID
Use:
    Chroma
    Weaviate
    Milvus
    pgvector

üìå RAG without metadata degrades fast.

3Ô∏è‚É£ Third question: Production or experimentation?
Experimentation / learning
Goals:
    Fast iteration
    Easy debugging
Use:
    Chroma
    pgvector
    FAISS Flat
Production systems
Goals:
    SLA(service level agreement)
    Uptime
    Scalability
Use:
    Pinecone
    Milvus
    Weaviate Cloud

4Ô∏è‚É£ Fourth question: Infra & Ops maturity
Low infra maturity
    Small team
    No SRE(site reliability engineering)
    Use:
        Pinecone
        Managed Weaviate
High infra maturity
    Kubernetes
    Monitoring
    On-call
Use:
    Milvus
    FAISS-based services
    Self-hosted Weaviate

5Ô∏è‚É£ Fifth question: Latency requirements
| Latency Target | Recommendation      |
| -------------- | ------------------- |
| <10 ms         | FAISS HNSW          |
| 10‚Äì50 ms       | Pinecone / Weaviate |
| 50‚Äì200 ms      | Chroma / pgvector   |
| Batch          | FAISS GPU           |


6Ô∏è‚É£ Golden rules (don‚Äôt violate these)

Rule 1Ô∏è‚É£
    One embedding model per collection / index
    Mixing models = meaningless similarity.
Rule 2Ô∏è‚É£
    Normalize if using cosine similarity
    Always.
Rule 3Ô∏è‚É£
    Index choice matters more than DB brand
    Bad index ‚Üí bad RAG.
Rule 4Ô∏è‚É£
    Retrieval quality > model size
    A smaller LLM + good retrieval
    beats
    a bigger LLM + bad retrieval.

7Ô∏è‚É£ Typical real-world stacks
Startup RAG
    Embeddings ‚Üí Pinecone ‚Üí GPT

Internal enterprise RAG
    Embeddings ‚Üí Weaviate ‚Üí Re-ranker ‚Üí LLM

Research / offline
    Embeddings ‚Üí FAISS ‚Üí Analysis

SQL-heavy org
    Embeddings ‚Üí pgvector ‚Üí LLM

-------------------------------------------------------------------------------------------------------------------------------------------------------------------
PART C ‚Äî RAG (Core System)

13Ô∏è‚É£ RAG architecture
13.1 Chunking strategies
13.2 Retrieval strategies
13.3 Augmented prompting
13.4 Context window budgeting
13.5 Failure modes

PART D ‚Äî Advanced RAG (Production)

14Ô∏è‚É£ Improvements & alternatives
14.1 Hybrid search
14.2 Re-ranking
14.3 Multi-query RAG
14.4 Agentic RAG
14.5 RAG vs Fine-tuning
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
PHASE 5: Agents (RAIN / AIRA level)

Goal: Production AI systems

1Ô∏è‚É£5Ô∏è‚É£ What is an agent

Tools vs agents

Decision loops

1Ô∏è‚É£6Ô∏è‚É£ Tool calling

Function schemas

Controlled outputs

1Ô∏è‚É£7Ô∏è‚É£ Memory types

Short-term (context)

Long-term (vector DB)

PHASE 6: Production & System Design

Goal: Real-world readiness

1Ô∏è‚É£8Ô∏è‚É£ FastAPI + LLM

API wrappers

Streaming via SSE

1Ô∏è‚É£9Ô∏è‚É£ Security

API keys

Environment variables

2Ô∏è‚É£0Ô∏è‚É£ Cost & performance

Tokens = money

Latency tradeoffs

2Ô∏è‚É£1Ô∏è‚É£ Evaluation & logging

Prompt versioning

Observability